import logging
import asyncio
import schedule
import time
from typing import List, Protocol
from prometheus_client import Counter, Histogram, start_http_server

from config_loader import load_config
from src.modules.2_colector.google_autocomplete import GoogleAutocompleteCollector
from src.modules.2_colector.google_trends import GoogleTrendsCollector
from src.modules.3_processing.processing import KeywordProcessor
from src.modules.1_data_base.queries import inserir_multiplas_palavras
from src.modules.4_api.send_to_api import APIClient

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üîß Carregamento de Configura√ß√£o
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CONFIG = load_config("config.v2.json", "config_schema.v2.json")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üìä M√âTRICAS (Prometheus)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PALAVRAS_PROCESSADAS = Counter("palavras_processadas_total", "Total de palavras processadas")
TEMPO_EXECUCAO_PIPELINE = Histogram("tempo_execucao_pipeline_segundos", "Tempo de execu√ß√£o do pipeline")
start_http_server(CONFIG.get("metrics", {}).get("port", 8000))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚öôÔ∏è LOGGING
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
log_cfg = CONFIG.get("logging", {})
log_level = getattr(logging, log_cfg.get("global_level", "INFO"))
logging.basicConfig(
    level=log_level,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/integrator.log"),
        logging.StreamHandler()
    ]
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üß© INTERFACE DE COLETORES
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class ColetorBase(Protocol):
    def coletar(self, nicho: str) -> List[str]: ...

class SystemIntegrator:
    def __init__(self, nicho: str, dry_run: bool = False):
        self.nicho = nicho
        self.dry_run = dry_run
        self.processor = KeywordProcessor()
        self.api_client = APIClient()
        self.coletores: List[ColetorBase] = [
            GoogleAutocompleteCollector(),
            GoogleTrendsCollector(),
        ]
        self.timeout_coleta = CONFIG["timeout"]
        self.timeout_processamento = 20
        self.timeout_storage = 10

    async def coletar_dados(self) -> List[str]:
        tasks = [asyncio.to_thread(c.coletar, self.nicho) for c in self.coletores]
        resultados = await asyncio.gather(*tasks, return_exceptions=True)
        palavras = []
        for result, coletor in zip(resultados, self.coletores):
            if isinstance(result, Exception):
                logging.error(f"‚ùå Erro em coletor {coletor.__class__.__name__}: {result}")
            else:
                palavras += result
        palavras = list(set(palavras))
        logging.info(f"üìä Total de palavras coletadas: {len(palavras)}")
        return palavras

    async def processar_dados(self, palavras: List[str]) -> List[str]:
        try:
            processadas = self.processor.processar_palavras(palavras)
            logging.info(f"üîç {len(processadas)} palavras ap√≥s processamento.")
            PALAVRAS_PROCESSADAS.inc(len(processadas))
            return processadas
        except Exception as e:
            logging.exception(f"‚ùå Erro ao processar palavras: {e}")
            return []

    async def armazenar_dados(self, palavras: List[str]):
        if self.dry_run:
            logging.info("üß™ [Dry-Run] Armazenamento simulado.")
            return
        try:
            if palavras:
                await inserir_multiplas_palavras(self.nicho, palavras)
                logging.info(f"üíæ {len(palavras)} palavras armazenadas no banco.")
            else:
                logging.warning("‚ö†Ô∏è Nenhuma palavra para armazenar.")
        except Exception as e:
            logging.exception(f"‚ùå Erro ao armazenar no banco: {e}")

    async def enviar_para_api(self, palavras: List[str]):
        if self.dry_run:
            logging.info("üß™ [Dry-Run] Envio para API simulado.")
            return
        try:
            if palavras:
                await self.api_client.enviar_para_api(self.nicho, palavras)
            else:
                logging.warning("‚ö†Ô∏è Nenhuma palavra para enviar √† API.")
        except Exception as e:
            logging.exception(f"‚ùå Erro ao enviar para API: {e}")

    @TEMPO_EXECUCAO_PIPELINE.time()
    async def executar_pipeline(self):
        logging.info(f"üöÄ Iniciando pipeline para o nicho '{self.nicho}' (dry_run={self.dry_run})")
        try:
            palavras = await asyncio.wait_for(self.coletar_dados(), timeout=self.timeout_coleta)
            palavras_processadas = await asyncio.wait_for(self.processar_dados(palavras), timeout=self.timeout_processamento)
            await asyncio.wait_for(self.armazenar_dados(palavras_processadas), timeout=self.timeout_storage)
            await asyncio.wait_for(self.enviar_para_api(palavras_processadas), timeout=self.timeout_storage)
            logging.info("‚úÖ Integra√ß√£o conclu√≠da com sucesso.")
        except asyncio.TimeoutError:
            logging.error("‚è±Ô∏è Pipeline cancelado por tempo excedido.")
        except Exception as e:
            logging.exception(f"‚ùå Erro inesperado no pipeline: {e}")