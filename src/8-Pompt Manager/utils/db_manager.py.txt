import sqlite3
import psycopg2
import logging
import redis
import json
import boto3
import threading
import time
import psutil
from typing import Optional, List, Dict, Any
from contextlib import contextmanager
from cryptography.fernet import Fernet
from src.config.config_loader import carregar_configuracoes

# ğŸ”¹ ConfiguraÃ§Ã£o de Logs
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("db_manager.log"), logging.StreamHandler()]
)

# ğŸ”¹ ConfiguraÃ§Ã£o do Redis para cache
redis_client = redis.Redis(host='localhost', port=6379, db=5, decode_responses=True)

# ğŸ”¹ ConfiguraÃ§Ã£o AWS S3 para backup do banco de dados
S3_BUCKET = "database-backups"
s3_client = boto3.client("s3")

# ğŸ”¹ ConfiguraÃ§Ã£o de Criptografia
SECRET_KEY = carregar_configuracoes().get("DB_SECRET_KEY", Fernet.generate_key().decode())
fernet = Fernet(SECRET_KEY.encode())

def criptografar_dado(dado: str) -> str:
    return fernet.encrypt(dado.encode()).decode()

def descriptografar_dado(dado: str) -> str:
    return fernet.decrypt(dado.encode()).decode()

# ğŸ”¹ Gerenciador de conexÃ£o para mÃºltiplos bancos
class DatabaseManager:
    def __init__(self, db_type: str = "sqlite"):
        self.config = carregar_configuracoes()
        self.db_type = db_type.lower()
        if self.db_type == "sqlite":
            self.db_path = self.config.get("SQLITE_DB_PATH", "database.db")
        elif self.db_type == "postgresql":
            self.db_config = self.config.get("POSTGRES_CONFIG", {})
            self.pool = psycopg2.pool.SimpleConnectionPool(minconn=1, maxconn=10, **self.db_config)
        else:
            raise ValueError("Tipo de banco de dados nÃ£o suportado")

    @contextmanager
    def connect(self):
        """Gerencia conexÃµes ao banco de dados de forma segura."""
        conn = None
        try:
            if self.db_type == "sqlite":
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
            elif self.db_type == "postgresql":
                conn = self.pool.getconn()
            yield conn
            conn.commit()
        except Exception as e:
            if conn:
                conn.rollback()
            logging.error(f"âŒ Erro no banco de dados: {e}")
        finally:
            if conn:
                if self.db_type == "postgresql":
                    self.pool.putconn(conn)
                else:
                    conn.close()

    def execute_query(self, query: str, params: Optional[tuple] = None) -> None:
        """Executa uma query no banco."""
        with self.connect() as conn:
            cursor = conn.cursor()
            cursor.execute(query, params or ())
            conn.commit()

    def fetch_data(self, query: str, params: Optional[tuple] = None) -> List[Dict[str, Any]]:
        """Executa uma query e retorna os resultados com cache."""
        cache_key = f"query:{query}:{json.dumps(params)}"
        cached_data = redis_client.get(cache_key)
        if cached_data:
            return json.loads(cached_data)
        
        with self.connect() as conn:
            cursor = conn.cursor()
            cursor.execute(query, params or ())
            results = [dict(zip([col[0] for col in cursor.description], row)) for row in cursor.fetchall()]
            redis_client.setex(cache_key, 600, json.dumps(results))  # Cache por 10 minutos
            return results

    def create_table(self, table_name: str, columns: Dict[str, str]) -> None:
        """Cria uma tabela dinamicamente, se nÃ£o existir."""
        columns_definition = ", ".join(f"{col} {dtype}" for col, dtype in columns.items())
        query = f"CREATE TABLE IF NOT EXISTS {table_name} ({columns_definition})"
        self.execute_query(query)
        logging.info(f"âœ… Tabela '{table_name}' verificada/criada com sucesso.")

# ğŸ”¹ Backup automÃ¡tico do banco de dados
def backup_database():
    """Cria e envia um backup do banco para o S3."""
    try:
        with open("database.db", "rb") as db_file:
            s3_client.put_object(Bucket=S3_BUCKET, Key=f"backup_{int(time.time())}.db", Body=db_file)
        logging.info("âœ… Backup do banco de dados enviado para o S3.")
    except Exception as e:
        logging.error(f"âŒ Erro ao criar backup do banco: {e}")

# ğŸ”¹ Monitoramento de mÃ©tricas do banco de dados
def monitorar_banco():
    """Monitora desempenho do banco e envia alertas se necessÃ¡rio."""
    while True:
        uso_cpu = psutil.cpu_percent()
        uso_memoria = psutil.virtual_memory().percent
        tamanho_banco = os.path.getsize("database.db") / (1024 * 1024)  # MB
        
        logging.info(f"ğŸ“Š CPU: {uso_cpu}%, MemÃ³ria: {uso_memoria}%, Tamanho do Banco: {tamanho_banco:.2f}MB")
        
        if uso_cpu > 85 or uso_memoria > 90 or tamanho_banco > 500:  # Alerta se banco > 500MB
            logging.warning("âš ï¸ ALERTA: PossÃ­vel sobrecarga no banco de dados!")
        
        time.sleep(600)  # A cada 10 minutos

# ğŸ”„ Agendamento de backup, limpeza do cache e monitoramento
def agendar_rotinas():
    while True:
        backup_database()
        redis_client.flushdb()
        logging.info("ğŸ”„ Cache do Redis limpo e backup do banco realizado.")
        time.sleep(3600)  # A cada 1 hora

# ğŸ”„ Iniciar threads para backup, limpeza e monitoramento
threading.Thread(target=agendar_rotinas, daemon=True).start()
threading.Thread(target=monitorar_banco, daemon=True).start()

# ğŸ”¹ Inicializar banco de dados e tabelas essenciais
db_manager = DatabaseManager("sqlite")
db_manager.create_table("usuarios", {"id": "INTEGER PRIMARY KEY", "nome": "TEXT", "email": "TEXT UNIQUE", "senha": "TEXT"})
