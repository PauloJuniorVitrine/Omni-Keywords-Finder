import sqlite3
import psycopg2
import logging
import redis
import json
import boto3
import threading
import time
import psutil
from typing import Optional, List, Dict, Any
from contextlib import contextmanager
from cryptography.fernet import Fernet
from src.config.config_loader import carregar_configuracoes

# 🔹 Configuração de Logs
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("db_manager.log"), logging.StreamHandler()]
)

# 🔹 Configuração do Redis para cache
redis_client = redis.Redis(host='localhost', port=6379, db=5, decode_responses=True)

# 🔹 Configuração AWS S3 para backup do banco de dados
S3_BUCKET = "database-backups"
s3_client = boto3.client("s3")

# 🔹 Configuração de Criptografia
SECRET_KEY = carregar_configuracoes().get("DB_SECRET_KEY", Fernet.generate_key().decode())
fernet = Fernet(SECRET_KEY.encode())

def criptografar_dado(dado: str) -> str:
    return fernet.encrypt(dado.encode()).decode()

def descriptografar_dado(dado: str) -> str:
    return fernet.decrypt(dado.encode()).decode()

# 🔹 Gerenciador de conexão para múltiplos bancos
class DatabaseManager:
    def __init__(self, db_type: str = "sqlite"):
        self.config = carregar_configuracoes()
        self.db_type = db_type.lower()
        if self.db_type == "sqlite":
            self.db_path = self.config.get("SQLITE_DB_PATH", "database.db")
        elif self.db_type == "postgresql":
            self.db_config = self.config.get("POSTGRES_CONFIG", {})
            self.pool = psycopg2.pool.SimpleConnectionPool(minconn=1, maxconn=10, **self.db_config)
        else:
            raise ValueError("Tipo de banco de dados não suportado")

    @contextmanager
    def connect(self):
        """Gerencia conexões ao banco de dados de forma segura."""
        conn = None
        try:
            if self.db_type == "sqlite":
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
            elif self.db_type == "postgresql":
                conn = self.pool.getconn()
            yield conn
            conn.commit()
        except Exception as e:
            if conn:
                conn.rollback()
            logging.error(f"❌ Erro no banco de dados: {e}")
        finally:
            if conn:
                if self.db_type == "postgresql":
                    self.pool.putconn(conn)
                else:
                    conn.close()

    def execute_query(self, query: str, params: Optional[tuple] = None) -> None:
        """Executa uma query no banco."""
        with self.connect() as conn:
            cursor = conn.cursor()
            cursor.execute(query, params or ())
            conn.commit()

    def fetch_data(self, query: str, params: Optional[tuple] = None) -> List[Dict[str, Any]]:
        """Executa uma query e retorna os resultados com cache."""
        cache_key = f"query:{query}:{json.dumps(params)}"
        cached_data = redis_client.get(cache_key)
        if cached_data:
            return json.loads(cached_data)
        
        with self.connect() as conn:
            cursor = conn.cursor()
            cursor.execute(query, params or ())
            results = [dict(zip([col[0] for col in cursor.description], row)) for row in cursor.fetchall()]
            redis_client.setex(cache_key, 600, json.dumps(results))  # Cache por 10 minutos
            return results

    def create_table(self, table_name: str, columns: Dict[str, str]) -> None:
        """Cria uma tabela dinamicamente, se não existir."""
        columns_definition = ", ".join(f"{col} {dtype}" for col, dtype in columns.items())
        query = f"CREATE TABLE IF NOT EXISTS {table_name} ({columns_definition})"
        self.execute_query(query)
        logging.info(f"✅ Tabela '{table_name}' verificada/criada com sucesso.")

# 🔹 Backup automático do banco de dados
def backup_database():
    """Cria e envia um backup do banco para o S3."""
    try:
        with open("database.db", "rb") as db_file:
            s3_client.put_object(Bucket=S3_BUCKET, Key=f"backup_{int(time.time())}.db", Body=db_file)
        logging.info("✅ Backup do banco de dados enviado para o S3.")
    except Exception as e:
        logging.error(f"❌ Erro ao criar backup do banco: {e}")

# 🔹 Monitoramento de métricas do banco de dados
def monitorar_banco():
    """Monitora desempenho do banco e envia alertas se necessário."""
    while True:
        uso_cpu = psutil.cpu_percent()
        uso_memoria = psutil.virtual_memory().percent
        tamanho_banco = os.path.getsize("database.db") / (1024 * 1024)  # MB
        
        logging.info(f"📊 CPU: {uso_cpu}%, Memória: {uso_memoria}%, Tamanho do Banco: {tamanho_banco:.2f}MB")
        
        if uso_cpu > 85 or uso_memoria > 90 or tamanho_banco > 500:  # Alerta se banco > 500MB
            logging.warning("⚠️ ALERTA: Possível sobrecarga no banco de dados!")
        
        time.sleep(600)  # A cada 10 minutos

# 🔄 Agendamento de backup, limpeza do cache e monitoramento
def agendar_rotinas():
    while True:
        backup_database()
        redis_client.flushdb()
        logging.info("🔄 Cache do Redis limpo e backup do banco realizado.")
        time.sleep(3600)  # A cada 1 hora

# 🔄 Iniciar threads para backup, limpeza e monitoramento
threading.Thread(target=agendar_rotinas, daemon=True).start()
threading.Thread(target=monitorar_banco, daemon=True).start()

# 🔹 Inicializar banco de dados e tabelas essenciais
db_manager = DatabaseManager("sqlite")
db_manager.create_table("usuarios", {"id": "INTEGER PRIMARY KEY", "nome": "TEXT", "email": "TEXT UNIQUE", "senha": "TEXT"})
